"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[7778],{9453:i=>{i.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"category","label":"ROS 2 Foundations","items":[{"type":"link","href":"/-physical-ai-humanoid-robotics-book/docs/ros2/01-introduction","label":"Intro to ROS 2","docId":"ros2/01-introduction","unlisted":false},{"type":"link","href":"/-physical-ai-humanoid-robotics-book/docs/ros2/02-nodes-topics","label":"Nodes & Topics","docId":"ros2/02-nodes-topics","unlisted":false},{"type":"link","href":"/-physical-ai-humanoid-robotics-book/docs/ros2/03-services-actions","label":"Services & Actions","docId":"ros2/03-services-actions","unlisted":false},{"type":"link","href":"/-physical-ai-humanoid-robotics-book/docs/ros2/04-urdf-modeling","label":"URDF Modeling","docId":"ros2/04-urdf-modeling","unlisted":false},{"type":"link","href":"/-physical-ai-humanoid-robotics-book/docs/ros2/05-rviz-visualization","label":"RViz2","docId":"ros2/05-rviz-visualization","unlisted":false},{"type":"link","href":"/-physical-ai-humanoid-robotics-book/docs/ros2/06-control-loops","label":"Control & Sensors","docId":"ros2/06-control-loops","unlisted":false},{"type":"link","href":"/-physical-ai-humanoid-robotics-book/docs/ros2/07-ai-bridge","label":"AI Bridge","docId":"ros2/07-ai-bridge","unlisted":false},{"type":"link","href":"/-physical-ai-humanoid-robotics-book/docs/ros2/08-mini-project","label":"Mini Project","docId":"ros2/08-mini-project","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Digital Twin","items":[{"type":"link","href":"/-physical-ai-humanoid-robotics-book/docs/digital-twin/01-intro-digital-twins","label":"Digital Twins Intro","docId":"digital-twin/01-intro-digital-twins","unlisted":false},{"type":"link","href":"/-physical-ai-humanoid-robotics-book/docs/digital-twin/02-physics-basics","label":"Physics Basics","docId":"digital-twin/02-physics-basics","unlisted":false},{"type":"link","href":"/-physical-ai-humanoid-robotics-book/docs/digital-twin/03-sensor-simulation","label":"Sensor Simulation","docId":"digital-twin/03-sensor-simulation","unlisted":false},{"type":"link","href":"/-physical-ai-humanoid-robotics-book/docs/digital-twin/04-humanoid-modeling","label":"Humanoid Models","docId":"digital-twin/04-humanoid-modeling","unlisted":false},{"type":"link","href":"/-physical-ai-humanoid-robotics-book/docs/digital-twin/05-environment-interaction","label":"Env & Interaction","docId":"digital-twin/05-environment-interaction","unlisted":false},{"type":"link","href":"/-physical-ai-humanoid-robotics-book/docs/digital-twin/06-unity-visualization","label":"Unity Visuals","docId":"digital-twin/06-unity-visualization","unlisted":false},{"type":"link","href":"/-physical-ai-humanoid-robotics-book/docs/digital-twin/07-best-practices","label":"Best Practices","docId":"digital-twin/07-best-practices","unlisted":false},{"type":"link","href":"/-physical-ai-humanoid-robotics-book/docs/digital-twin/08-linking-modules","label":"Module Connections","docId":"digital-twin/08-linking-modules","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"NVIDIA Isaac Sim","items":[{"type":"link","href":"/-physical-ai-humanoid-robotics-book/docs/isaac-sim/introduction","label":"01 - Introduction to NVIDIA Isaac Sim","docId":"isaac-sim/introduction","unlisted":false},{"type":"link","href":"/-physical-ai-humanoid-robotics-book/docs/isaac-sim/simulation-setup","label":"02 - Simulation Setup and Synthetic Data","docId":"isaac-sim/simulation-setup","unlisted":false},{"type":"link","href":"/-physical-ai-humanoid-robotics-book/docs/isaac-sim/perception-pipelines","label":"03 - Advanced Perception Pipelines (VSLAM)","docId":"isaac-sim/perception-pipelines","unlisted":false},{"type":"link","href":"/-physical-ai-humanoid-robotics-book/docs/isaac-sim/navigation-planning","label":"04 - Navigation and Path Planning (Nav2)","docId":"isaac-sim/navigation-planning","unlisted":false},{"type":"link","href":"/-physical-ai-humanoid-robotics-book/docs/isaac-sim/reinforcement-learning","label":"05 - Reinforcement Learning for Humanoid Control","docId":"isaac-sim/reinforcement-learning","unlisted":false},{"type":"link","href":"/-physical-ai-humanoid-robotics-book/docs/isaac-sim/integrating-modules","label":"06 - Integrating AI Modules","docId":"isaac-sim/integrating-modules","unlisted":false},{"type":"link","href":"/-physical-ai-humanoid-robotics-book/docs/isaac-sim/best-practices","label":"07 - Best Practices and Optimization","docId":"isaac-sim/best-practices","unlisted":false},{"type":"link","href":"/-physical-ai-humanoid-robotics-book/docs/isaac-sim/preparing-module4","label":"08 - Preparing for Module 4 (Vision-Language-Action)","docId":"isaac-sim/preparing-module4","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"VLA Robotics LLM","items":[{"type":"link","href":"/-physical-ai-humanoid-robotics-book/docs/vla/foundations-vla","label":"01 - Foundations of Vision-Language-Action (VLA)","docId":"vla/foundations-vla","unlisted":false},{"type":"link","href":"/-physical-ai-humanoid-robotics-book/docs/vla/whisper-language","label":"02 - Whisper + Language Understanding","docId":"vla/whisper-language","unlisted":false},{"type":"link","href":"/-physical-ai-humanoid-robotics-book/docs/vla/cognitive-planning","label":"03 - Cognitive Planning with Large Language Models (LLMs)","docId":"vla/cognitive-planning","unlisted":false},{"type":"link","href":"/-physical-ai-humanoid-robotics-book/docs/vla/vision-perception","label":"04 - Vision and Depth Perception for VLA","docId":"vla/vision-perception","unlisted":false},{"type":"link","href":"/-physical-ai-humanoid-robotics-book/docs/vla/ros2-action-generation","label":"05 - ROS 2 Action Generation for VLA","docId":"vla/ros2-action-generation","unlisted":false},{"type":"link","href":"/-physical-ai-humanoid-robotics-book/docs/vla/integrated-pipeline-examples","label":"06 - Integrated Pipeline Examples","docId":"vla/integrated-pipeline-examples","unlisted":false},{"type":"link","href":"/-physical-ai-humanoid-robotics-book/docs/vla/capstone-preparation","label":"07 - Capstone Preparation: \\"The Autonomous Humanoid\\"","docId":"vla/capstone-preparation","unlisted":false}],"collapsed":true,"collapsible":true}]},"docs":{"chatbot":{"id":"chatbot","title":"Chatbot Integration Guide","description":"How to use and configure the integrated chatbot in Docusaurus."},"digital-twin/01-intro-digital-twins":{"id":"digital-twin/01-intro-digital-twins","title":"Introduction to Digital Twins","description":"Defining the concept and scope of Digital Twins for humanoid robotics.","sidebar":"tutorialSidebar"},"digital-twin/02-physics-basics":{"id":"digital-twin/02-physics-basics","title":"Physics Simulation Basics","description":"Understanding rigid body dynamics, gravity, and friction in robot simulation.","sidebar":"tutorialSidebar"},"digital-twin/03-sensor-simulation":{"id":"digital-twin/03-sensor-simulation","title":"Sensors in Simulation","description":"Modeling LiDAR, Depth Cameras, RGB cameras, and IMUs in simulated environments.","sidebar":"tutorialSidebar"},"digital-twin/04-humanoid-modeling":{"id":"digital-twin/04-humanoid-modeling","title":"Humanoid Modeling in Gazebo","description":"Specific considerations for simulating humanoid robots in Gazebo, including inertia and balancing.","sidebar":"tutorialSidebar"},"digital-twin/05-environment-interaction":{"id":"digital-twin/05-environment-interaction","title":"Environment and Interaction","description":"Modeling terrain, static objects, and dynamic obstacles for humanoid robot simulations.","sidebar":"tutorialSidebar"},"digital-twin/06-unity-visualization":{"id":"digital-twin/06-unity-visualization","title":"Unity Visualization","description":"Utilizing Unity for high-fidelity visualization and human-robot interaction in digital twins.","sidebar":"tutorialSidebar"},"digital-twin/07-best-practices":{"id":"digital-twin/07-best-practices","title":"Simulation Best Practices","description":"Identifying common simulation errors, understanding the Sim-to-Real gap, and managing latency.","sidebar":"tutorialSidebar"},"digital-twin/08-linking-modules":{"id":"digital-twin/08-linking-modules","title":"Linking Modules","description":"Understanding the connections between Module 1 (ROS 2), Module 2 (Digital Twin), and Module 3 (Isaac Sim).","sidebar":"tutorialSidebar"},"isaac-sim/best-practices":{"id":"isaac-sim/best-practices","title":"07 - Best Practices and Optimization","description":"This chapter consolidates best practices for developing and optimizing robotics applications within NVIDIA Isaac Sim, with a particular focus on principles essential for successful Sim-to-Real transfer. We will delve into techniques like domain randomization, discuss strategies for bridging the reality gap, and provide insights into optimizing simulation performance.","sidebar":"tutorialSidebar"},"isaac-sim/integrating-modules":{"id":"isaac-sim/integrating-modules","title":"06 - Integrating AI Modules","description":"This chapter focuses on the holistic integration of various AI modules\u2014perception, navigation, and control\u2014into a cohesive system for autonomous robotics within NVIDIA Isaac Sim. We will illustrate how these disparate components work together, forming the \\"AI-robot brain,\\" and discuss the principles behind designing robust integrated systems.","sidebar":"tutorialSidebar"},"isaac-sim/introduction":{"id":"isaac-sim/introduction","title":"01 - Introduction to NVIDIA Isaac Sim","description":"This chapter provides an introduction to NVIDIA Isaac Sim, a powerful robotics simulation platform. We will explore its core concepts, architecture, and the foundational technologies it leverages to enable advanced robotics development. Isaac Sim, built on NVIDIA Omniverse, offers a highly realistic and extensible environment for designing, testing, and deploying AI-powered robots.","sidebar":"tutorialSidebar"},"isaac-sim/navigation-planning":{"id":"isaac-sim/navigation-planning","title":"04 - Navigation and Path Planning (Nav2)","description":"This chapter focuses on integrating the ROS 2 Navigation Stack (Nav2) with NVIDIA Isaac Sim. We will explore how to set up Nav2, understand the role of costmaps, and configure global and local planners to enable autonomous navigation for robots within simulated environments.","sidebar":"tutorialSidebar"},"isaac-sim/perception-pipelines":{"id":"isaac-sim/perception-pipelines","title":"03 - Advanced Perception Pipelines (VSLAM)","description":"In this chapter, we delve into advanced perception techniques, specifically focusing on Visual Simultaneous Localization and Mapping (VSLAM) within NVIDIA Isaac Sim. We will explore how Isaac ROS components facilitate the integration of VSLAM pipelines, enabling robots to understand their environment and localize themselves effectively using visual data.","sidebar":"tutorialSidebar"},"isaac-sim/preparing-module4":{"id":"isaac-sim/preparing-module4","title":"08 - Preparing for Module 4 (Vision-Language-Action)","description":"As we conclude our exploration of NVIDIA Isaac Sim, this chapter serves as a bridge to Module 4, which will delve into Vision-Language-Action (VLA) systems for advanced robotic intelligence. The concepts and skills acquired throughout this module, particularly in simulation, perception, and control, lay the essential groundwork for understanding and implementing VLA.","sidebar":"tutorialSidebar"},"isaac-sim/reinforcement-learning":{"id":"isaac-sim/reinforcement-learning","title":"05 - Reinforcement Learning for Humanoid Control","description":"This chapter explores the application of Reinforcement Learning (RL) within NVIDIA Isaac Sim to train humanoid robots for complex control tasks. We will delve into the core concepts of RL, strategies for reward shaping, and effective task design for achieving robust manipulation and locomotion behaviors in simulated humanoids.","sidebar":"tutorialSidebar"},"isaac-sim/simulation-setup":{"id":"isaac-sim/simulation-setup","title":"02 - Simulation Setup and Synthetic Data","description":"This chapter guides you through the fundamental steps of setting up a basic simulation environment within NVIDIA Isaac Sim. We will cover the essential components for scene creation, object manipulation, and initiating basic environment interactions. A core focus will also be on the generation of synthetic data, a critical capability for AI-driven robotics development.","sidebar":"tutorialSidebar"},"ros2/01-introduction":{"id":"ros2/01-introduction","title":"Introduction to ROS 2","description":"Understanding the Robot Operating System and setting up your environment.","sidebar":"tutorialSidebar"},"ros2/02-nodes-topics":{"id":"ros2/02-nodes-topics","title":"Nodes and Topics","description":"Creating your first ROS 2 nodes in Python.","sidebar":"tutorialSidebar"},"ros2/03-services-actions":{"id":"ros2/03-services-actions","title":"Services and Actions","description":"Understanding synchronous and asynchronous communication in ROS 2.","sidebar":"tutorialSidebar"},"ros2/04-urdf-modeling":{"id":"ros2/04-urdf-modeling","title":"URDF Robot Modeling","description":"Describing your robot\'s physical structure with URDF.","sidebar":"tutorialSidebar"},"ros2/05-rviz-visualization":{"id":"ros2/05-rviz-visualization","title":"RViz2 Visualization","description":"Visualizing your URDF robot model in RViz2.","sidebar":"tutorialSidebar"},"ros2/06-control-loops":{"id":"ros2/06-control-loops","title":"Control Loops and Sensors","description":"Implementing a basic control loop and integrating sensor data.","sidebar":"tutorialSidebar"},"ros2/07-ai-bridge":{"id":"ros2/07-ai-bridge","title":"AI Agent to ROS 2 Bridge","description":"Bridging external AI agents with the ROS 2 ecosystem.","sidebar":"tutorialSidebar"},"ros2/08-mini-project":{"id":"ros2/08-mini-project","title":"Mini Project - Humanoid Nervous System","description":"Integrating all components to build a basic humanoid nervous system.","sidebar":"tutorialSidebar"},"vla/capstone-preparation":{"id":"vla/capstone-preparation","title":"07 - Capstone Preparation: \\"The Autonomous Humanoid\\"","description":"This concluding chapter for Module 4 bridges to the Capstone project, \\"The Autonomous Humanoid,\\" consolidating Vision-Language-Action (VLA) principles. It guides designing and implementing an end-to-end VLA system, revisiting key concepts, and providing considerations for setting up a challenging yet feasible Capstone environment based on our design decisions.","sidebar":"tutorialSidebar"},"vla/cognitive-planning":{"id":"vla/cognitive-planning","title":"03 - Cognitive Planning with Large Language Models (LLMs)","description":"This chapter explores how Large Language Models (LLMs) and Vision-Language Models (VLMs) enable cognitive planning in humanoid robots. We delve into hierarchical planning, where LLMs generate high-level strategies from natural language commands, and discuss critical trade-offs of local versus cloud-based LLM inference.","sidebar":"tutorialSidebar"},"vla/foundations-vla":{"id":"vla/foundations-vla","title":"01 - Foundations of Vision-Language-Action (VLA)","description":"This chapter introduces Vision-Language-Action (VLA) systems for humanoid robots, exploring the motivation for integrating vision, language, and action to enable intelligent human-robot interaction. It covers the high-level architecture of a complete VLA pipeline, setting the stage for subsequent detailed discussions.","sidebar":"tutorialSidebar"},"vla/integrated-pipeline-examples":{"id":"vla/integrated-pipeline-examples","title":"06 - Integrated Pipeline Examples","description":"This chapter provides concrete, worked examples of Vision-Language-Action (VLA) behaviors, demonstrating the seamless integration of components discussed previously. We detail 2-3 scenarios, illustrating the flow from natural language command to robot execution, including ROS 2 action breakdowns and conceptual message flow diagrams.","sidebar":"tutorialSidebar"},"vla/ros2-action-generation":{"id":"vla/ros2-action-generation","title":"05 - ROS 2 Action Generation for VLA","description":"This chapter focuses on the Action component of Vision-Language-Action (VLA) systems, detailing how Large Language Models (LLMs) translate cognitive plans into executable robotic behaviors using the ROS 2 Action interface. We cover both standard ROS 2 actions (Nav2, MoveIt) and custom action definitions for complex VLA tasks.","sidebar":"tutorialSidebar"},"vla/vision-perception":{"id":"vla/vision-perception","title":"04 - Vision and Depth Perception for VLA","description":"This chapter focuses on vision and depth perception\'s vital role in grounding Vision-Language-Action (VLA) systems in the physical world. We explore how robots \\"see\\" and interpret surroundings, emphasizing NVIDIA Isaac Perception models for object understanding and comparing depth-based and SLAM-based approaches for spatial awareness.","sidebar":"tutorialSidebar"},"vla/whisper-language":{"id":"vla/whisper-language","title":"02 - Whisper + Language Understanding","description":"This chapter details the first critical step in a Vision-Language-Action (VLA) pipeline: ingesting and understanding human language. We focus on OpenAI\'s Whisper model for robust speech-to-text (STT) transcription and introduce natural language understanding (NLU) to convert raw text into actionable intent for a humanoid robot.","sidebar":"tutorialSidebar"}}}}')}}]);