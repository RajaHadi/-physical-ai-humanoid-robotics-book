"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[5808],{1752:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>t,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"vla/cognitive-planning","title":"03 - Cognitive Planning with Large Language Models (LLMs)","description":"This chapter explores how Large Language Models (LLMs) and Vision-Language Models (VLMs) enable cognitive planning in humanoid robots. We delve into hierarchical planning, where LLMs generate high-level strategies from natural language commands, and discuss critical trade-offs of local versus cloud-based LLM inference.","source":"@site/docs/vla/03-cognitive-planning.md","sourceDirName":"vla","slug":"/vla/cognitive-planning","permalink":"/-physical-ai-humanoid-robotics-book/docs/vla/cognitive-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/RajaHadi/-physical-ai-humanoid-robotics-book/tree/main/docs/vla/03-cognitive-planning.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"02 - Whisper + Language Understanding","permalink":"/-physical-ai-humanoid-robotics-book/docs/vla/whisper-language"},"next":{"title":"04 - Vision and Depth Perception for VLA","permalink":"/-physical-ai-humanoid-robotics-book/docs/vla/vision-perception"}}');var r=i(4848),a=i(8453);const o={},t="03 - Cognitive Planning with Large Language Models (LLMs)",l={},c=[{value:"3.1 LLMs as Cognitive Planners in Robotics",id:"31-llms-as-cognitive-planners-in-robotics",level:2},{value:"3.2 Hierarchical Planning for Robustness and Safety",id:"32-hierarchical-planning-for-robustness-and-safety",level:2},{value:"3.3 Local vs. Cloud LLM Inference for Robotics",id:"33-local-vs-cloud-llm-inference-for-robotics",level:2},{value:"3.4 Representing Plans: The ROS 2 Action Graph",id:"34-representing-plans-the-ros-2-action-graph",level:2},{value:"3.5 Feedback and Re-planning",id:"35-feedback-and-re-planning",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.header,{children:(0,r.jsx)(e.h1,{id:"03---cognitive-planning-with-large-language-models-llms",children:"03 - Cognitive Planning with Large Language Models (LLMs)"})}),"\n",(0,r.jsx)(e.p,{children:"This chapter explores how Large Language Models (LLMs) and Vision-Language Models (VLMs) enable cognitive planning in humanoid robots. We delve into hierarchical planning, where LLMs generate high-level strategies from natural language commands, and discuss critical trade-offs of local versus cloud-based LLM inference."}),"\n",(0,r.jsx)(e.h2,{id:"31-llms-as-cognitive-planners-in-robotics",children:"3.1 LLMs as Cognitive Planners in Robotics"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Beyond Simple Commands"}),": LLMs interpret complex, nuanced natural language instructions, inferring user intent from ambiguous commands\u2014crucial for intuitive human-robot interaction."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Cognitive Planning"}),': An LLM breaks down high-level goals (e.g., "Clean the room") into executable sub-goals or actions, reasoning about task decomposition, state transitions, and environmental obstacles.']}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Contextual Understanding"}),": LLMs incorporate dialogue history, world knowledge, and real-time perception data (from Chapter 04) for grounded decisions."]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"32-hierarchical-planning-for-robustness-and-safety",children:"3.2 Hierarchical Planning for Robustness and Safety"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Decision"}),": Focus on ",(0,r.jsx)(e.strong,{children:"hierarchical planning with LLMs for robotics"}),"."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Concept"}),": The LLM acts as a high-level strategist, receiving a command and outputting an abstract plan (e.g., sequence of sub-goals). This plan is then translated into concrete, low-level ROS 2 actions by a separate robotic planning system (task planner, state machine)."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Advantages"}),":","\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Robustness & Safety"}),": LLM influence is constrained by well-tested robotic control systems, preventing unsafe direct commands."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Explainability"}),": Abstract plans provide human-readable reasoning traces, simplifying debugging."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Leverages Existing Robotics"}),": Integrates seamlessly with ROS 2 frameworks (Nav2, MoveIt) for motion planning and collision checking."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Scalability"}),": LLMs excel at long-horizon tasks by breaking them into manageable sub-goals."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Comparison to Direct ROS Action Generation"}),": Direct generation (LLM outputs raw ROS commands) is less robust and has higher safety risks due to lack of intermediary checks and need for fine-grained LLM control knowledge."]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"33-local-vs-cloud-llm-inference-for-robotics",children:"3.3 Local vs. Cloud LLM Inference for Robotics"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Decision"}),": Emphasize ",(0,r.jsx)(e.strong,{children:"local LLM inference for critical real-time and privacy-sensitive tasks"}),", while discussing cloud and hybrid approaches."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Local LLM Inference"}),":","\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Pros"}),": Significantly lower latency (no network delays) for real-time control; high privacy (data stays on device); offline operation."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Cons"}),": Requires powerful, expensive onboard hardware; limits LLM size/complexity."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Cloud LLM Inference"}),":","\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Pros"}),": Access to larger, more powerful, and updated LLMs; lower upfront hardware costs."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Cons"}),": Introduces network latency (unsuitable for real-time/safety-critical tasks); raises data privacy concerns; incurs high, unpredictable recurring costs; requires network connectivity."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Hybrid Approaches"}),": Combine local (small, optimized LLMs for real-time) and cloud (large LLMs for complex, less time-critical reasoning) to leverage strengths of both."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Implications for Robotics"}),": Local inference is generally preferred for humanoid robots requiring immediate reactions, low-latency control, and sensitive data handling."]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"34-representing-plans-the-ros-2-action-graph",children:"3.4 Representing Plans: The ROS 2 Action Graph"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Structured Output"}),': The LLM\'s cognitive plan must translate into a structured, machine-readable format for robot execution, often an "action graph."']}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Action Graph Concept"}),": A conceptual representation outlining a sequence of ROS 2 actions (standard and custom, Chapter 05) with interdependencies, parameters, and conditional branches.","\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Nodes"}),": Individual ROS 2 actions (e.g., ",(0,r.jsx)(e.code,{children:"NavigateToPose"}),")."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Edges"}),": Control flow (sequential, parallel, conditional execution)."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Parameters"}),": Data extracted from LLM's understanding and environment."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Custom Message Types"}),": Defining custom ROS 2 message types (e.g., ",(0,r.jsx)(e.code,{children:"vla_msgs/ActionGraph"}),") encapsulates the action graph for standardized communication."]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"35-feedback-and-re-planning",children:"3.5 Feedback and Re-planning"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Continuous Feedback Loop"}),": The LLM planner continuously receives updates from perception (Chapter 04) and action execution (Chapter 05)."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Error Handling and Adaptation"}),": Feedback is crucial for:","\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Monitoring Progress"}),": Tracking plan execution status."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Detecting Failures"}),": Identifying failed sub-actions or unexpected outcomes."]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Dynamic Re-planning"}),": Interpreting failures or environmental changes to modify plans or request user clarification."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Goal Monitoring"}),": The LLM continuously monitors whether the overall high-level goal is being achieved."]}),"\n"]})]})}function h(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>o,x:()=>t});var s=i(6540);const r={},a=s.createContext(r);function o(n){const e=s.useContext(a);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function t(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:o(n.components),s.createElement(a.Provider,{value:e},n.children)}}}]);