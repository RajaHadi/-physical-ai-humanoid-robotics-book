"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[3288],{3153:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>s,contentTitle:()=>l,default:()=>p,frontMatter:()=>t,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"vla/integrated-pipeline-examples","title":"06 - Integrated Pipeline Examples","description":"This chapter provides concrete, worked examples of Vision-Language-Action (VLA) behaviors, demonstrating the seamless integration of components discussed previously. We detail 2-3 scenarios, illustrating the flow from natural language command to robot execution, including ROS 2 action breakdowns and conceptual message flow diagrams.","source":"@site/docs/vla/06-integrated-pipeline-examples.md","sourceDirName":"vla","slug":"/vla/integrated-pipeline-examples","permalink":"/-physical-ai-humanoid-robotics-book/docs/vla/integrated-pipeline-examples","draft":false,"unlisted":false,"editUrl":"https://github.com/RajaHadi/-physical-ai-humanoid-robotics-book/tree/main/docs/vla/06-integrated-pipeline-examples.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"05 - ROS 2 Action Generation for VLA","permalink":"/-physical-ai-humanoid-robotics-book/docs/vla/ros2-action-generation"},"next":{"title":"07 - Capstone Preparation: \\"The Autonomous Humanoid\\"","permalink":"/-physical-ai-humanoid-robotics-book/docs/vla/capstone-preparation"}}');var a=i(4848),r=i(8453);const t={},l="06 - Integrated Pipeline Examples",s={},c=[{value:"6.1 Example 1: &quot;Pick Up the Red Cup&quot;",id:"61-example-1-pick-up-the-red-cup",level:2},{value:"6.1.1 Scenario Description",id:"611-scenario-description",level:3},{value:"6.1.2 VLA Pipeline Flow and ROS 2 Breakdown",id:"612-vla-pipeline-flow-and-ros-2-breakdown",level:3},{value:"6.1.3 Conceptual Message Flow Diagram",id:"613-conceptual-message-flow-diagram",level:3},{value:"6.2 Example 2: &quot;Clean the Room&quot;",id:"62-example-2-clean-the-room",level:2},{value:"6.2.1 Scenario Description",id:"621-scenario-description",level:3},{value:"6.2.2 VLA Pipeline Flow and ROS 2 Breakdown",id:"622-vla-pipeline-flow-and-ros-2-breakdown",level:3},{value:"6.2.3 Conceptual Message Flow Diagram",id:"623-conceptual-message-flow-diagram",level:3},{value:"6.3 Example 3: &quot;Follow Me&quot; (Conceptual)",id:"63-example-3-follow-me-conceptual",level:2},{value:"6.3.1 Scenario Description",id:"631-scenario-description",level:3},{value:"6.3.2 VLA Pipeline Flow and ROS 2 Breakdown",id:"632-vla-pipeline-flow-and-ros-2-breakdown",level:3},{value:"6.3.3 Conceptual Message Flow Diagram",id:"633-conceptual-message-flow-diagram",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"06---integrated-pipeline-examples",children:"06 - Integrated Pipeline Examples"})}),"\n",(0,a.jsx)(n.p,{children:"This chapter provides concrete, worked examples of Vision-Language-Action (VLA) behaviors, demonstrating the seamless integration of components discussed previously. We detail 2-3 scenarios, illustrating the flow from natural language command to robot execution, including ROS 2 action breakdowns and conceptual message flow diagrams."}),"\n",(0,a.jsx)(n.h2,{id:"61-example-1-pick-up-the-red-cup",children:'6.1 Example 1: "Pick Up the Red Cup"'}),"\n",(0,a.jsx)(n.p,{children:"This foundational VLA task integrates speech recognition, LLM planning, visual perception, navigation, and manipulation."}),"\n",(0,a.jsx)(n.h3,{id:"611-scenario-description",children:"6.1.1 Scenario Description"}),"\n",(0,a.jsx)(n.p,{children:'A user commands, "Robot, please pick up the red cup from the table." The robot must hear, understand, plan, navigate, visually confirm, and grasp the cup.'}),"\n",(0,a.jsx)(n.h3,{id:"612-vla-pipeline-flow-and-ros-2-breakdown",children:"6.1.2 VLA Pipeline Flow and ROS 2 Breakdown"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Voice Command (Microphone)"}),": Human speaks the command."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Speech-to-Text (Whisper)"}),": ",(0,a.jsx)(n.code,{children:"vla_stt_node"}),' transcribes audio to "pick up the red cup" on ',(0,a.jsx)(n.code,{children:"/vla/voice_command"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Cognitive Planning (LLM Planner)"}),": ",(0,a.jsx)(n.code,{children:"vla_llm_planner_node"})," interprets the command, queries ",(0,a.jsx)(n.code,{children:"/vla/detected_objects"}),' for "red cup," determines its 3D pose, and generates a ',(0,a.jsx)(n.strong,{children:"hierarchical plan"}),". This plan decomposes the command into a ",(0,a.jsx)(n.strong,{children:"conceptual action graph"})," of ROS 2 actions.","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Generated Plan"}),": ",(0,a.jsx)(n.code,{children:"NavigateToPose"})," (near cup) -> ",(0,a.jsx)(n.code,{children:"PerceptionQuery"})," (confirm cup) -> ",(0,a.jsx)(n.code,{children:"PickUpObject"})," (custom action)."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Output"}),": ROS 2 Topic ",(0,a.jsx)(n.code,{children:"/vla/action_plan"}),"."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Action Execution (High-Level Executive)"}),": ",(0,a.jsx)(n.code,{children:"vla_executive_node"})," interprets ",(0,a.jsx)(n.code,{children:"/vla/action_plan"}),".","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Execution"}),": Calls Nav2 ",(0,a.jsx)(n.code,{children:"NavigateToPose"}),". Upon arrival, may issue ",(0,a.jsx)(n.code,{children:"PerceptionQuery"}),". Calls custom ",(0,a.jsx)(n.code,{children:"PickUpObject"})," (internally orchestrates MoveIt for arm/gripper)."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Output"}),": Publishes status on ",(0,a.jsx)(n.code,{children:"/vla/executive_status"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"613-conceptual-message-flow-diagram",children:"6.1.3 Conceptual Message Flow Diagram"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-mermaid",children:"graph TD\r\n    A[Human Voice Command] --\x3e B(Microphone)\r\n    B --\x3e C{vla_stt_node<br>(Whisper STT)}\r\n    C -- transcribed_text (/vla/voice_command) --\x3e D{vla_llm_planner_node<br>(LLM Cognitive Planning)}\r\n    subgraph Robot Perception\r\n        E[Camera/Depth Sensor] --\x3e F{vla_perception_node<br>(Isaac Perception/SLAM)}\r\n        F -- detected_objects (/vla/detected_objects) --\x3e D\r\n    end\r\n    D -- action_plan (/vla/action_plan) --\x3e G{vla_executive_node<br>(Action Execution Engine)}\r\n    G -- NavigateToPose(Goal) --\x3e H[Nav2 Action Server]\r\n    G -- PerceptionQuery(Service/Action) --\x3e F\r\n    G -- PickUpObject(Goal) --\x3e I[Custom PickUpObject Action Server]\r\n    I -- MoveGroup(Goal) --\x3e J[MoveIt2 Action Server]\r\n    H --\x3e K(Humanoid Robot<br>Execution)\r\n    J --\x3e K\r\n    K -- robot_state (/odom, /joint_states) --\x3e D\r\n    K -- sensor_feedback --\x3e F\n"})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"62-example-2-clean-the-room",children:'6.2 Example 2: "Clean the Room"'}),"\n",(0,a.jsx)(n.p,{children:"This complex VLA behavior involves high-level planning and sequential execution in a multi-room environment with static and dynamic obstacles."}),"\n",(0,a.jsx)(n.h3,{id:"621-scenario-description",children:"6.2.1 Scenario Description"}),"\n",(0,a.jsx)(n.p,{children:'User instructs, "Robot, please clean the room. Put all the blocks in the bin and all the cups on the shelf." The robot must hear, understand, generate a comprehensive hierarchical plan, and execute it, adapting to new perception data.'}),"\n",(0,a.jsx)(n.h3,{id:"622-vla-pipeline-flow-and-ros-2-breakdown",children:"6.2.2 VLA Pipeline Flow and ROS 2 Breakdown"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Voice Command & STT"}),': "clean the room..." is transcribed.']}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Cognitive Planning (LLM Planner)"}),": Interprets the command, performs ",(0,a.jsx)(n.strong,{children:"hierarchical planning"}),' (e.g., "Explore -> pick/place blocks -> pick/place cups"). Plan accounts for re-planning if objects move.',"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Generated Plan"}),": Iterative calls to ",(0,a.jsx)(n.code,{children:"ExploreRoom"}),", ",(0,a.jsx)(n.code,{children:"PickUpObject"}),", ",(0,a.jsx)(n.code,{children:"NavigateToPose"}),", ",(0,a.jsx)(n.code,{children:"PlaceObject"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Output"}),": ",(0,a.jsx)(n.code,{children:"/vla/action_plan"}),"."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Action Execution (High-Level Executive)"}),": Interprets the Action Graph.","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Execution"}),": Calls custom ",(0,a.jsx)(n.code,{children:"ExploreRoom"}),". Iteratively calls ",(0,a.jsx)(n.code,{children:"PickUpObject"}),", ",(0,a.jsx)(n.code,{children:"NavigateToPose"}),", ",(0,a.jsx)(n.code,{children:"PlaceObject"}),". Each step uses ",(0,a.jsx)(n.strong,{children:"perceptual grounding"}),". Nav2 handles local replanning for dynamic obstacles; LLM executive may re-evaluate."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Output"}),': Publishes status (e.g., "Cleaning progress").']}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"623-conceptual-message-flow-diagram",children:"6.2.3 Conceptual Message Flow Diagram"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-mermaid",children:"graph TD\r\n    A[Human Voice Command] --\x3e B(Microphone)\r\n    B --\x3e C{vla_stt_node<br>(Whisper STT)}\r\n    C -- transcribed_text (/vla/voice_command) --\x3e D{vla_llm_planner_node<br>(LLM Cognitive Planning)}\r\n    subgraph Robot Perception\r\n        E[Camera/Depth Sensor] --\x3e F{vla_perception_node<br>(Isaac Perception/SLAM)}\r\n        F -- detected_objects (/vla/detected_objects) --\x3e D\r\n    end\r\n    D -- action_plan (/vla/action_plan) --\x3e G{vla_executive_node<br>(Action Execution Engine)}\r\n    G -- ExploreRoom(Goal) --\x3e H[Custom ExploreRoom Action Server]\r\n    G -- PickUpObject(Goal) --\x3e I[Custom PickUpObject Action Server]\r\n    G -- PlaceObject(Goal) --\x3e J[Custom PlaceObject Action Server]\r\n    I --\x3e K(Humanoid Robot<br>Execution)\r\n    J --\x3e K\r\n    H --\x3e K\r\n    K -- robot_state, sensor_feedback --\x3e F\r\n    K -- robot_state (/odom, /joint_states) --\x3e D\r\n    style G fill:#f9f,stroke:#333,stroke-width:2px\r\n    style D fill:#f9f,stroke:#333,stroke-width:2px\r\n    style F fill:#f9f,stroke:#333,stroke-width:2px\r\n    style C fill:#f9f,stroke:#333,stroke-width:2px\n"})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"63-example-3-follow-me-conceptual",children:'6.3 Example 3: "Follow Me" (Conceptual)'}),"\n",(0,a.jsx)(n.p,{children:"This VLA behavior relies heavily on continuous perception, dynamic re-planning, and user tracking."}),"\n",(0,a.jsx)(n.h3,{id:"631-scenario-description",children:"6.3.1 Scenario Description"}),"\n",(0,a.jsx)(n.p,{children:'User instructs, "Robot, follow me." The robot must identify, continuously track the user, maintain safe distance, and navigate dynamically.'}),"\n",(0,a.jsx)(n.h3,{id:"632-vla-pipeline-flow-and-ros-2-breakdown",children:"6.3.2 VLA Pipeline Flow and ROS 2 Breakdown"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Voice Command & STT"}),': "Follow me" is transcribed.']}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Cognitive Planning (LLM Planner)"}),': Interprets "follow me" as a continuous goal. Queries perception for the human and generates a ',(0,a.jsx)(n.strong,{children:"continuous hierarchical plan"}),': "Track human," "Maintain following distance," "Navigate safely."',"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Generated Plan"}),": Custom ",(0,a.jsx)(n.code,{children:"FollowHuman"})," action."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Output"}),": ",(0,a.jsx)(n.code,{children:"/vla/action_plan"}),"."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Action Execution (High-Level Executive)"}),": Calls custom ",(0,a.jsx)(n.code,{children:"FollowHuman"})," action client.","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Execution"}),": ",(0,a.jsx)(n.code,{children:"FollowHuman"})," server continuously subscribes to ",(0,a.jsx)(n.code,{children:"/vla/detected_objects"})," for user's updated 3D pose. It calculates a target pose for the robot and calls Nav2 ",(0,a.jsx)(n.code,{children:"NavigateToPose"}),". Nav2 handles local obstacle avoidance; the executive adapts if the user moves unexpectedly."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Output"}),": Provides continuous feedback on tracking status."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"633-conceptual-message-flow-diagram",children:"6.3.3 Conceptual Message Flow Diagram"}),"\n",(0,a.jsx)(n.p,{children:"(Similar to previous examples, emphasizing the dynamic loop between perception, planning, and execution, with continuous feedback driving navigation goals.)"}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.p,{children:"These examples demonstrate how VLA components integrate to enable complex, natural language-driven robotic behaviors. The LLM's ability to translate high-level human intent into structured, executable robotic plans, grounded in reality through real-time perception and executed via ROS 2 actions, enables robust and adaptive robot behavior."})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>l});var o=i(6540);const a={},r=o.createContext(a);function t(e){const n=o.useContext(r);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:t(e.components),o.createElement(r.Provider,{value:n},e.children)}}}]);