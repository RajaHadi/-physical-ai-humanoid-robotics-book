# Data Model: Module 4 - Vision-Language-Action (VLA) for Humanoid Robotics

This document outlines the key entities involved in the Vision-Language-Action (VLA) pipeline for humanoid robots, as described in Module 4. These are conceptual entities representing the core components and data flows within the system.

## Core Entities

-   **Humanoid Robot**:
    *   **Description**: The central embodied agent that interacts with the environment. It receives commands, perceives its surroundings, plans actions, and executes them. This entity encompasses the physical robot and its control systems.
    *   **Key Attributes (Conceptual)**:
        *   `current_pose`: (Position and orientation in the environment)
        *   `joint_states`: (Current configuration of robotic joints)
        *   `sensor_data`: (Streams from various sensors)
        *   `capabilities`: (e.g., navigation, manipulation, speech)
    *   **Relationships**: Interacts with Perception Sensors, executes ROS 2 Actions.

-   **Natural Language Command**:
    *   **Description**: High-level, human-issued instructions provided to the robot, typically in spoken form (voice input). This is the initial input to the VLA pipeline.
    *   **Key Attributes (Conceptual)**:
        *   `text_content`: (Transcribed natural language text)
        *   `source_audio`: (Raw audio input)
        *   `timestamp`: (Time of command reception)
    *   **Relationships**: Processed by Whisper, interpreted by LLM.

-   **Whisper (AI Model)**:
    *   **Description**: An AI model responsible for transcribing spoken natural language commands into text. It acts as the initial perception layer for auditory input.
    *   **Key Attributes (Conceptual)**:
        *   `model_version`: (e.g., tiny, base, small)
        *   `transcription_output`: (Text conversion of audio)
        *   `confidence_score`: (Confidence in transcription accuracy)
    *   **Relationships**: Processes Natural Language Command (audio), outputs text for LLM.

-   **LLM (Large Language Model) / VLM (Vision-Language Model)**:
    *   **Description**: The core AI component responsible for understanding natural language commands, performing cognitive planning, and generating structured action sequences. VLMs extend this by incorporating visual understanding.
    *   **Key Attributes (Conceptual)**:
        *   `model_id`: (Identifier for the specific LLM/VLM used)
        *   `interpreted_intent`: (High-level goal extracted from command)
        *   `cognitive_plan`: (Structured sequence of actions to achieve intent)
        *   `context_history`: (Previous interactions for conversational coherence)
    *   **Relationships**: Interprets Natural Language Command (text), generates Action Graph/Plan, utilizes Perception Sensors data for grounding.

-   **Perception Sensors**:
    *   **Description**: Devices that provide the robot with environmental data, enabling it to understand its surroundings, identify objects, and localize itself. Examples include RGB-D cameras and LiDAR.
    *   **Key Attributes (Conceptual)**:
        *   `sensor_type`: (e.g., RGB-D Camera, LiDAR, IMU)
        *   `data_stream`: (Raw sensor data: images, depth maps, point clouds)
        *   `processed_data`: (Object detections, segmentations, 3D poses)
    *   **Relationships**: Provides input to LLM/VLM for grounding, supports Navigation, enables Manipulation.

-   **ROS 2 Actions/Topics/Services**:
    *   **Description**: The standardized communication framework within the robot operating system for inter-module communication and executing structured robotic behaviors. Actions are used for long-running, feedback-enabled tasks.
    *   **Key Attributes (Conceptual)**:
        *   `action_goal`: (Desired state or task for a robotic behavior)
        *   `action_feedback`: (Continuous updates on task progress)
        *   `action_result`: (Final outcome of a task)
        *   `message_types`: (Standardized data structures for communication)
    *   **Relationships**: Executes Action Graph/Plan components, receives commands from LLM.

-   **Action Graph/Plan**:
    *   **Description**: A structured, usually sequential, representation of robotic actions derived from the LLM's cognitive planning. This graph details the steps a robot needs to take to fulfill a Natural Language Command.
    *   **Key Attributes (Conceptual)**:
        *   `sequence_of_actions`: (Ordered list of ROS 2 Actions or high-level steps)
        *   `preconditions`: (Conditions required before an action can execute)
        *   `postconditions`: (Expected state after an action executes)
        *   `current_status`: (Execution status of the plan)
    *   **Relationships**: Generated by LLM, executed via ROS 2 Actions.

-   **Environment**:
    *   **Description**: The physical or simulated space in which the humanoid robot operates. Contains static and dynamic elements.
    *   **Key Attributes (Conceptual)**:
        *   `map_data`: (Global or local occupancy grids, feature maps)
        *   `static_obstacles`: (Immovable objects)
        *   `dynamic_obstacles`: (Moving entities)
        *   `objects_of_interest`: (Items for manipulation, identification)
    *   **Relationships**: Perceived by Perception Sensors, navigated by Humanoid Robot.
